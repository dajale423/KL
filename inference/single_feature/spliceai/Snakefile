import os
import sys
import glob
import numpy as np
import pandas as pd
import math
import sys
import random
import pickle
import csv

import dask.dataframe as dd
import dask.array as da
from dask.distributed import Client

import torch
import pyro
import pyro.distributions as dist
import pyro.distributions.constraints as constraints
from pyro.nn import PyroModule

from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader

sys.path.insert(0, '/home/djl34/lab_pd/kl/git/KL/scripts')
import raklette
from run_raklette import run_raklette
from run_raklette import run_raklette_cov
from run_raklette import TSVDataset
# from snakefile_raklette import snakefile_raklette_cov
# from snakefile_raklette import snakefile_make_sample

sys.path.insert(0, '/home/djl34/lab_pd/simulator/code')
from others import round_sig


include: "/home/djl34/lab_pd/kl/git/KL/scripts/Snakefile"


factor = 1

def get_mem_mb(wildcards, attempt):
    return attempt * 20000 * factor

###################################################################################################################

KL_data_dir = "/home/djl34/lab_pd/kl/data"
scratch_dir = "/n/scratch3/users/d/djl34"

base_set = ["A", "C", "T", "G"]
chrom_set = [str(x) for x in range(1, 23)]
chrom_set = ["2"]
# chrom_set = ["-2"]
# chrom_set = ["22"]


even_chrom_set = [str(2 * x) for x in range(1, 12)]
# chrom_set = even_chrom_set

# cutoff_list = [0, 1, 2, 3, 4, 5]

wildcard_constraints:
    chrom="\d+",
    chrom_all="[-+]?\d+",
    epoch="\d+",
    interval_min="[+-]?([0-9]*[.])?[0-9]+",
    interval_max="[+-]?([0-9]*[.])?[0-9]+",
    samplesize="\d+",

file_directory = "single_feature/spliceai/"

###################################################################################################################

rule all:
    input:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "chr_22.tsv"),
#         os.path.join(scratch_dir, "kl_input/" + file_directory + "footprints_dhs_gene_chr_22.tsv"),
#         os.path.join(scratch_dir, "kl_input/" + file_directory + "footprints_dhs_gene_chr_-2/chunk_100000_0.tsv")
        [os.path.join(KL_data_dir, "raklette_output/" + file_directory + "DS_gene_chr_" + chrom + "_chunksize_1000000_covonly_lr_0.01_gamma_0.5_epoch_150_covprior_0.1.pkl") for chrom in chrom_set],
#         [os.path.join(KL_data_dir, "raklette_output/" + file_directory + "footprints_far_tss_sample_10000000_chr_" + chrom + "_chunksize_100000_covonly_lr_0.01_gamma_0.5_epoch_10_covprior_0.1.pkl") for chrom in chrom_set],
        
########################################## getting column of interest ##############################################

rule make_tsv_file:
    input:
        os.path.join(KL_data_dir, "whole_genome/spliceai/{chrom}/_metadata")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "chr_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:

            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")

            print(rate.columns)

            include_columns = ["mu_index", "Freq_bin_adaptive"]
            feature_list = ['spliceai_gene','DS', 'DS_AL', 'DS_DL', 'DS_AG', 'DS_DG']
            include_columns.extend(feature_list)

            for feature in feature_list:
                rate[feature] = rate[feature].fillna(0)
                
            rate = rate[include_columns]
            
            rate = rate.rename(columns={"Freq_bin_adaptive": "Freq_bin"})  
            
            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
            

rule high_DS_gene:
    input:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "chr_{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "DS_gene_chr_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:

            rate = dd.read_csv(input[0], sep = "\t", dtype={'spliceai_gene': 'object'})
            
            rate["DS_high"] = rate["DS"].where((rate["DS"] > 0.1), 0)

            cov_columns = ["DS_high"]
            include_columns = ["mu_index", "Freq_bin"]
            include_columns.extend(cov_columns)

            rate = rate[include_columns]

            for column in cov_columns:
                rate[column] = rate[column].astype(bool).astype(int)

            rate = rate[rate[cov_columns[0]] == 1]

            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
            
rule footprints_far_tss_only_gene:
    input:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "footprints_tss_chr_{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "footprints_far_tss_chr_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:

            rate = dd.read_csv(input[0], sep = "\t")
            
            rate["footprints_far"] = rate["footprint_mean_signal"].where((rate["footprint_mean_signal"] > 0) & (rate["TSS_Distance"] > 5000), 0)

            cov_columns = ["footprints_far"]
            include_columns = ["mu_index", "Freq_bin"]
            include_columns.extend(cov_columns)

            rate = rate[include_columns]

            for column in cov_columns:
                rate[column] = rate[column].astype(bool).astype(int)

            rate = rate[rate[cov_columns[0]] == 1]

            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
            

