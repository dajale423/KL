import os
import sys
import glob
import numpy as np
import pandas as pd
import math
import sys
import random
import pickle
import csv

import dask.dataframe as dd
from dask.distributed import Client

import torch
import pyro
import pyro.distributions as dist
import pyro.distributions.constraints as constraints
from pyro.nn import PyroModule

from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader

sys.path.insert(0, '/home/djl34/lab_pd/kl/git/KL/scripts')
import raklette
from run_raklette import run_raklette
from run_raklette import run_raklette_cov
from run_raklette import TSVDataset

sys.path.insert(0, '/home/djl34/lab_pd/simulator/code')
from others import round_sig

def get_mem_mb(wildcards, attempt):
    return attempt * 25000

###################################################################################################################

KL_data_dir = "/home/djl34/lab_pd/kl/data"
scratch_dir = "/n/scratch3/users/d/djl34"

base_set = ["A", "C", "T", "G"]
chrom_set = [str(x) for x in range(1, 23)]
chrom_set = ["22"]
chrom_set = ["-2"]
even_chrom_set = [str(2 * x) for x in range(1, 12)]
chrom_set = even_chrom_set

# cutoff_list = [0, 1, 2, 3, 4, 5]

wildcard_constraints:
    chrom="[-+]?\d+",
    epoch="\d+",
    interval_min="[+-]?([0-9]*[.])?[0-9]+",
    interval_max="[+-]?([0-9]*[.])?[0-9]+",
    samplesize="\d+",

file_directory = "single_feature/neutral/"


###################################################################################################################

header = "_nogenes"

rule all:
    input:
        [os.path.join(KL_data_dir, "raklette_output/" + file_directory + chrom + header +"_sample_10000000_covonly_lr_0.01_gamma_0.5_chunksize_10000_epoch_1_covprior_0.1.pkl") for chrom in chrom_set],

        
########################################## zoonomia genes ##############################################

rule make_tsv_files:
    input:
        os.path.join(KL_data_dir, "whole_genome/exon/{chrom}/_metadata")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=4,
        mem_mb=get_mem_mb
    run:
        with Client() as client:

            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")

            print(rate.columns)

            include_columns = ["mu_index", "Freq_bin", "Neutral"]
            
            rate["Neutral"] = rate["Neutral"].fillna(0)

            rate = rate[include_columns]
            
            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)

rule exclude_nongenes:
    input:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "{chrom}_nogenes.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=4,
        mem_mb=get_mem_mb
    run:
        with Client() as client:

            rate = dd.read_csv(input, sep = "\t")
            
            rate = rate[rate["Neutral"] == 1]
            
            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
            
            
rule make_tsv_files_even:
    input:
        [os.path.join(scratch_dir, "kl_input/" + file_directory + chrom + ".tsv") for chrom in even_chrom_set]
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "-2.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=25000
    run:
        with Client() as client:

            rate = dd.read_csv(input, sep = "\t")

            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)

rule sample_size:
    input:
        os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}_sample_{samplesize}_sampled.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=25000
    run:
        with Client() as client:
            rate = dd.read_csv(input[0], sep = "\t")
            
            sites = len(rate)
            
            if sites != 0:
                rate = rate.sample(frac = int(wildcards.samplesize)/sites, replace = True)
            
            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
            
rule sample_size_get_exact_line:
    input:
        os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}_sample_{samplesize}_sampled.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}_sample_{samplesize}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=1,
        mem_mb=5000
    run:
        head_value = int(wildcards.samplesize) + 1
        
        shell("head -n {head_value} {input} >> {output}")
            
                
rule get_length:
    input:
        os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}_length.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=1,
        mem_mb=2000
    run:
        with Client() as client:
            rate = dd.read_csv(input[0], sep = "\t")
            
            length = len(rate)
            column_length = len(rate.columns)
            
            f = open(output[0], "w")
            
            f.write(str(length) + "\n")
            f.write(str(column_length) + "\n")
            
            f.close()
           
                
########################################## For running KL analysis #####################################
        
rule run_KL_cov:
    input:
        variants = os.path.join(scratch_dir, "kl_input/" + file_directory + "{header}.tsv"),
        length_file = os.path.join(scratch_dir, "kl_input/" + file_directory +"{header}_length.tsv"),
        neutral_sfs = KL_data_dir + "/whole_genome/neutral/5_bins/all.tsv",
#         neutral_sfs = KL_data_dir + "/whole_genome/neutral/5_bins/all_original.tsv"
    output:
        os.path.join(KL_data_dir, "raklette_output/"+ file_directory +"{header}_covonly_lr_{learning_rate}_gamma_{gamma}_chunksize_{chunksize}_epoch_{epoch}_covprior_{cov_prior}.pkl")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=20000
    run:
        n_covs = 1
        
        input_filename = input.variants
        output_filename = output[0]
        neutral_sfs_filename = input.neutral_sfs
        
        
        df = pd.read_csv(input.length_file, sep = "\t", header = None)
        nb_samples = df[0][0]
        nb_features = df[0][1] - 2
        
        print("number of samples: " + str(nb_samples))
        
        if nb_samples == 0:
            f = open(output_filename, "w")
            f.write("no sample")
            f.close()
        else:        
            with open(input.variants) as f:
                first_line = f.readline()
            header = first_line.split("\t")
            
            chunksize = int(wildcards.chunksize)

            print("number of chunks " + str(nb_samples/chunksize))

            dataset = TSVDataset(input_filename, chunksize=chunksize, nb_samples = nb_samples, header_all = header, features = header)
            loader = DataLoader(dataset, batch_size=1, num_workers=1, shuffle=False)

            num_epochs = int(wildcards.epoch)
            cov_prior = float(wildcards.cov_prior)
            
            #lets run raklette
            run_raklette_cov(loader, nb_features, num_epochs, neutral_sfs_filename, output_filename, 
                         float(wildcards.learning_rate), float(wildcards.gamma), 
                             cov_sigma_prior = torch.tensor(cov_prior, dtype=torch.float32))
