{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad947aa",
   "metadata": {},
   "source": [
    "## Here, I will try to apply Evan's KL analysis and convert it to a more friendly framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12251b94",
   "metadata": {},
   "source": [
    "## There are functions to include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee76011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.constraints as constraints\n",
    "\n",
    "import torch.distributions.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e8a10a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformations of the SFS\n",
    "####################################################\n",
    "def multinomial_trans(sfs_probs, offset=None):\n",
    "    sfs_probs = np.array(sfs_probs)\n",
    "    P_0 = sfs_probs[...,0]\n",
    "    if offset:\n",
    "        betas = np.log(sfs_probs[...,1:]) - np.log(P_0[...,None]) - offset\n",
    "    else:\n",
    "        betas = np.log(sfs_probs[...,1:]) - np.log(P_0[...,None])\n",
    "    return betas\n",
    "\n",
    "def multinomial_trans_torch(sfs_probs):\n",
    "    P_0 = sfs_probs[...,0]\n",
    "    return torch.log(sfs_probs[...,1:]) - torch.log(P_0[...,None])\n",
    "\n",
    "def KL_fw(neut_probs, sel_probs):\n",
    "    return np.sum(neut_probs * (np.log(neut_probs) - np.log(sel_probs)), axis=-1)\n",
    "\n",
    "def KL_rv(neut_probs, sel_probs):\n",
    "    return np.sum(sel_probs * (np.log(sel_probs) - np.log(neut_probs)), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c2c59",
   "metadata": {},
   "source": [
    "## Change Evan's Code to class format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KL_inference(PyroModule):\n",
    "    def __init__(self, neut_sfs_full, n_genes, n_covs, n_bins, mu_ref, \n",
    "                             sample_sfs=None, n_mix=2, cov_sigma_prior=torch.tensor(0.1, dtype=torch.float32), trans=\"abs\", pdist=\"t\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        ##define useful transformations\n",
    "        self.pad = torch.nn.ConstantPad1d((1,0), 0.)            # Add a 0 to a tensor\n",
    "#         self.softmax = PyroModule[nn.Softmax]()\n",
    "        self.softmax = torch.nn.Softmax(-1)                     # softmax transform along the last dimension\n",
    "        relu = torch.nn.ReLU()                             # map everything < 0 -> 0 \n",
    "\n",
    "        #other definitions\n",
    "        beta_neut_full = multinomial_trans_torch(neut_sfs_full) #neut_sfs_full is the neutral sfs\n",
    "        beta_neut = beta_neut_full[ref_mu_ii,:]\n",
    "        self.beta_neut = beta_neut\n",
    "        \n",
    "        self.mu_ref = mu_ref\n",
    "        \n",
    "        ## Setup flexible prior\n",
    "        # parameters describing the prior over genes are set as pyro.param, meaning they will get point estimates (no posterior)\n",
    "        if pdist==\"t\":\n",
    "            # t-distribution can modulate covariance (L) and kurtosis (df)\n",
    "            # uses a fixed \"point mass\" at zero as one of the mixtures, not sure if this should be kept\n",
    "            beta_prior_mean = pyro.param(\"beta_prior_mean\", torch.randn((n_mix-1,n_bins)),\n",
    "                                         constraint=constraints.real)\n",
    "            beta_prior_L = pyro.param(\"beta_prior_L\", torch.linalg.cholesky(0.01*torch.diag(torch.ones(n_bins, dtype=torch.float32))).expand(n_mix-1, n_bins, n_bins), \n",
    "                                                                            constraint=constraints.lower_cholesky)\n",
    "            beta_prior_df = pyro.param(\"beta_prior_df\", torch.tensor([10]*(n_mix-1), dtype=torch.float32), constraint=constraints.positive)\n",
    "            mix_probs = pyro.param(\"mix_probs\", torch.ones(n_mix, dtype=torch.float32)/n_mix, constraint=constraints.simplex)\n",
    "        elif pdist==\"normal\":\n",
    "            # normal model has zero covariance, a different variance for each bin though\n",
    "            mix_probs = pyro.param(\"mix_probs\", torch.ones(n_mix, dtype=torch.float32)/n_mix, constraint=constraints.simplex)\n",
    "            beta_prior_loc = pyro.param(\"beta_prior_loc\", torch.randn((n_mix, n_bins)), constraint=constraints.real)\n",
    "            beta_prior_scale = pyro.param(\"beta_prior_scale\", torch.rand((n_mix, n_bins)), constraint=constraints.positive)\n",
    "\n",
    "        # interaction term bewteen gene-based selection and mutation rate\n",
    "        self.beta_prior_b = pyro.param(\"beta_prior_b\", torch.tensor([0.001]*n_bins, dtype=torch.float32), constraint=constraints.positive)\n",
    "\n",
    "        # Each covariate has a vector of betas, one for each bin, maybe think about different prior here?\n",
    "#         with pyro.plate(\"covariates\", n_covs):\n",
    "#             beta_cov = pyro.sample(\"beta_cov\", dist.HalfCauchy(cov_sigma_prior).expand([n_bins]).to_event(1))\n",
    "        self.beta_cov = pyro.sample(\"beta_cov\", dist.HalfCauchy(0.1).expand([n_bins, n_covs]).to_event(2))\n",
    "\n",
    "        with pyro.plate(\"genes\", n_genes):\n",
    "            # sample latent betas from either t or normal distribution\n",
    "            if pdist==\"t\":\n",
    "                beta_sel = pyro.sample(\"beta_sel\", dist.MixtureSameFamily(dist.Categorical(mix_probs),\n",
    "                                       dist.MultivariateStudentT(df=torch.cat((beta_prior_df, torch.tensor([1000], dtype=torch.float32))), \n",
    "                                                                 loc=torch.cat((beta_prior_mean, \n",
    "                                                                                torch.tensor([0]*n_bins, dtype=torch.float32).expand((1, n_bins)))), \n",
    "                                                                 scale_tril=torch.cat((beta_prior_L, \n",
    "                                                                                       torch.linalg.cholesky(torch.diag(1e-8*torch.ones(n_bins, dtype=torch.float32))).expand(1, n_bins, n_bins))))))\n",
    "            elif pdist==\"normal\":\n",
    "                beta_sel = pyro.sample(\"beta_sel\", dist.MixtureSameFamily(dist.Categorical(mix_probs),\n",
    "                                                                          dist.Normal(beta_prior_loc, beta_prior_scale).to_event(1)))\n",
    "            # apply transform to latent betas\n",
    "            if trans == \"abs\":\n",
    "                self.beta_trans = torch.cumsum(torch.abs(beta_sel), dim=-1)\n",
    "            elif trans==\"logabs\":\n",
    "                self.beta_trans = torch.cumsum(torch.log(torch.abs(beta_sel)+1), dim=-1)\n",
    "            elif trans==\"relu\":\n",
    "                self.beta_trans = torch.cumsum(relu(beta_sel), dim=-1)\n",
    "            elif trans==\"logrelu\":\n",
    "                self.beta_trans = torch.cumsum(torch.log(relu(beta_sel)+1), dim=-1)\n",
    "                        \n",
    "    def forward(self, mu_vals, gene_ids, covariates, y=None):\n",
    "        \n",
    "        # calculate the multinomial coefficients for each gene and each mutation rate\n",
    "        mu_adj = self.mu_ref[...,None] * torch.cumsum(self.beta_prior_b, -1) * self.beta_trans[...,None,:]\n",
    "        mn_sfs = (self.beta_neut  - \n",
    "                  self.beta_trans[...,None,:] -\n",
    "                  mu_adj)\n",
    "        \n",
    "        # convert to probabilities per-site and adjust for covariates\n",
    "        linear = self.pad(mn_sfs[..., gene_ids, mu_vals, :] - torch.matmul(covariates, torch.cumsum(self.beta_cov, -1)))\n",
    "        sfs = self.softmax(linear)\n",
    "        \n",
    "#         linear = torch.matmul(x, torch.cumsum(self.beta_cov, -1))        \n",
    "#         sfs = self.softmax(neutral_sfs - linear).squeeze(-1)\n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", \n",
    "                              dist.Categorical(probs=sfs),\n",
    "                              obs=y.squeeze())\n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392e618",
   "metadata": {},
   "source": [
    "## inference portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62880bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variables\n",
    "#define neut_sfs_full, mu_vals, gene_ids, covariates\n",
    "n_covs = covariates.shape[-1]          # number of covariates included\n",
    "n_genes = len(torch.unique(gene_ids))  # number of genes\n",
    "\n",
    "#define model and guide\n",
    "model = KL_inference(neut_sfs_full, n_genes, n_covs, n_bins, mu_ref)\n",
    "guide = pyro.infer.autoguide.AutoNormal(model)\n",
    "\n",
    "#run inference\n",
    "pyro.clear_param_store()\n",
    "# run SVI\n",
    "adam = pyro.optim.Adam({\"lr\":lr})\n",
    "elbo = pyro.infer.Trace_ELBO(num_particles=num_particles, vectorize_particles=True)\n",
    "svi = pyro.infer.SVI(mu_sfs_sitewise_regr_cov, guide, adam, elbo)\n",
    "losses = []\n",
    "for step in tqdm(range(n_steps)): # tqdm is just a progress bar thing \n",
    "    loss = svi.step(mu_vals, gene_ids, covariates, sample_sfs)\n",
    "    losses.append(loss)\n",
    "fig, ax = plot_losses(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216ab94e",
   "metadata": {},
   "source": [
    "## post inference portion (same as raklette_daniel.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af2042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab gene-DFE prior parameter point estimates\n",
    "beta_neut = beta_neut_full[ref_mu_ii,:]\n",
    "if pdist==\"t\":\n",
    "    beta_prior_df = pyro.param(\"beta_prior_df\")\n",
    "    beta_prior_mean = pyro.param(\"beta_prior_mean\")\n",
    "    beta_prior_L = pyro.param(\"beta_prior_L\")\n",
    "    mix_probs = pyro.param(\"mix_probs\")\n",
    "elif pdist==\"normal\":\n",
    "    mix_probs = pyro.param(\"mix_probs\")\n",
    "    beta_prior_loc = pyro.param(\"beta_prior_loc\")\n",
    "    beta_prior_scale = pyro.param(\"beta_prior_scale\")\n",
    "\n",
    "beta_prior_b = pyro.param(\"beta_prior_b\")\n",
    "\n",
    "# Sample betas from the DFE prior, representing the fit distribution across genes\n",
    "if pdist==\"t\":\n",
    "    prior_dist = dist.MixtureSameFamily(dist.Categorical(mix_probs),\n",
    "                                          dist.MultivariateStudentT(df=torch.cat((beta_prior_df, torch.tensor([1000], dtype=float))), \n",
    "                                                                loc=torch.cat((beta_prior_mean, \n",
    "                                                                                   torch.tensor([0]*n_bins, dtype=float).expand((1, n_bins)))), \n",
    "                                                                scale_tril=torch.cat((beta_prior_L, \n",
    "                                                                            torch.linalg.cholesky(torch.diag(1e-8*torch.ones(n_bins, dtype=float))).expand(1, n_bins, n_bins))))\n",
    "                                                             )\n",
    "elif pdist==\"normal\":\n",
    "    prior_dist = dist.MixtureSameFamily(dist.Categorical(mix_probs),\n",
    "                                        dist.Normal(beta_prior_loc, beta_prior_scale).to_event(1))\n",
    "prior_samps = prior_dist.sample((post_samps,))\n",
    "\n",
    "if trans == \"abs\":\n",
    "    prior_trans = torch.cumsum(torch.abs(prior_samps), axis=-1)\n",
    "elif trans==\"logabs\":\n",
    "    prior_trans = torch.cumsum(torch.log(torch.abs(prior_samps)+1), axis=-1)\n",
    "elif trans==\"relu\":\n",
    "    prior_trans = torch.cumsum(relu(prior_samps), axis=-1)\n",
    "elif trans==\"logrelu\":\n",
    "    prior_trans = torch.cumsum(torch.log(relu(prior_samps)+1), axis=-1)\n",
    "\n",
    "## Prior SFS probabilities for gene effects in the absence of covariates\n",
    "prior_probs = softmax(pad(beta_neut - prior_trans -\n",
    "                          mu_ref[ref_mu_ii]*torch.cumsum(beta_prior_b, -1)*prior_trans\n",
    "                         )\n",
    "                     ).detach().numpy()\n",
    "\n",
    "# take samples from the posterior distribution on all betas\n",
    "with pyro.plate(\"samples\", post_samps, dim=-2):\n",
    "    post_samples = guide()\n",
    "\n",
    "if pdist==\"t\":\n",
    "    result = {\"neut_sfs_full\":neut_sfs_full, \"beta_neut_full\":beta_neut_full, \"ref_mu_ii\":ref_mu_ii,\n",
    "              \"beta_prior_df\":beta_prior_df, \"beta_prior_mean\":beta_prior_mean, \"beta_prior_L\":beta_prior_L,\n",
    "              \"mix_probs\":mix_probs, \n",
    "              \"beta_prior_b\":beta_prior_b, \"trans\":trans,\n",
    "              \"prior_probs\":prior_probs, \"post_samples\":post_samples, \"mu_ref\":mu_ref}\n",
    "elif pdist==\"normal\":\n",
    "    result = {\"neut_sfs_full\":neut_sfs_full, \"beta_neut_full\":beta_neut_full, \"ref_mu_ii\":ref_mu_ii,\n",
    "              \"beta_prior_scale\":beta_prior_scale, \"beta_prior_loc\":beta_prior_loc,\n",
    "              \"mix_probs\":mix_probs,\n",
    "              \"beta_prior_b\":beta_prior_b, \"trans\":trans,\n",
    "              \"prior_probs\":prior_probs, \"post_samples\":post_samples, \"mu_ref\":mu_ref}\n",
    "\n",
    "# calculate the posterior distribution on KL for each gene\n",
    "result = calc_KL_genewise(result, pdist=pdist)\n",
    "\n",
    "## Then calculate the posteriors for covariate betas\n",
    "result[\"post_beta_cov\"] = torch.cumsum(post_samples['beta_cov'], -1)\n",
    "result[\"losses\"] = losses\n",
    "result[\"fig\"] = (fig, ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
