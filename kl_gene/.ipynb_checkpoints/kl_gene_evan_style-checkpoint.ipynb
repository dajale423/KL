{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad947aa",
   "metadata": {},
   "source": [
    "## Run KL analysis for intergenic region, footprinting region, and DHS sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c3db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## write function for model\n",
    "\n",
    "def model(beta_neut, mu_vals, gene_ids, covariates, n_bins, mu_ref, \n",
    "                             sample_sfs=None, n_mix=2, cov_sigma_prior=torch.tensor(0.1, dtype=torch.float32), trans=\"abs\", pdist=\"t\"):\n",
    "    \"\"\"\n",
    "    Pyro sampling model for a gene-based DFE with covariates\n",
    "    \"\"\"\n",
    "    n_covs = covariates.shape[-1]          # number of covariates included\n",
    "    n_sites = len(mu_vals)                 # number of sites (potential mutations) we are modeling\n",
    "    n_genes = len(torch.unique(gene_ids))  # number of genes\n",
    "    mu = torch.unique(mu_vals)             # set of all possible mutation rates\n",
    "    n_mu = len(mu)                         # number of unique mutation rates\n",
    "    \n",
    "    ## Setup flexible prior\n",
    "    # parameters describing the prior over genes are set as pyro.param, meaning they will get point estimates (no posterior)\n",
    "    if pdist==\"t\":\n",
    "        # t-distribution can modulate covariance (L) and kurtosis (df)\n",
    "        # uses a fixed \"point mass\" at zero as one of the mixtures, not sure if this should be kept\n",
    "        beta_prior_mean = pyro.param(\"beta_prior_mean\", torch.randn((n_mix-1,n_bins)),\n",
    "                                     constraint=constraints.real)\n",
    "        beta_prior_L = pyro.param(\"beta_prior_L\", torch.linalg.cholesky(0.01*torch.diag(torch.ones(n_bins, dtype=torch.float32))).expand(n_mix-1, n_bins, n_bins), \n",
    "                                                                        constraint=constraints.lower_cholesky)\n",
    "        beta_prior_df = pyro.param(\"beta_prior_df\", torch.tensor([10]*(n_mix-1), dtype=torch.float32), constraint=constraints.positive)\n",
    "        mix_probs = pyro.param(\"mix_probs\", torch.ones(n_mix, dtype=torch.float32)/n_mix, constraint=constraints.simplex)\n",
    "    elif pdist==\"normal\":\n",
    "        # normal model has zero covariance, a different variance for each bin though\n",
    "        mix_probs = pyro.param(\"mix_probs\", torch.ones(n_mix, dtype=torch.float32)/n_mix, constraint=constraints.simplex)\n",
    "        beta_prior_loc = pyro.param(\"beta_prior_loc\", torch.randn((n_mix, n_bins)), constraint=constraints.real)\n",
    "        beta_prior_scale = pyro.param(\"beta_prior_scale\", torch.rand((n_mix, n_bins)), constraint=constraints.positive)\n",
    "        \n",
    "    # interaction term bewteen gene-based selection and mutation rate\n",
    "    beta_prior_b = pyro.param(\"beta_prior_b\", torch.tensor([0.001]*n_bins, dtype=torch.float32), constraint=constraints.positive)\n",
    "    \n",
    "    # Each covariate has a vector of betas, one for each bin, maybe think about different prior here?\n",
    "    with pyro.plate(\"covariates\", n_covs):\n",
    "        beta_cov = pyro.sample(\"beta_cov\", dist.HalfCauchy(cov_sigma_prior).expand([n_bins]).to_event(1))\n",
    "    \n",
    "    with pyro.plate(\"genes\", n_genes):\n",
    "        # sample latent betas from either t or normal distribution\n",
    "        if pdist==\"t\":\n",
    "            beta_sel = pyro.sample(\"beta_sel\", dist.MixtureSameFamily(dist.Categorical(mix_probs),\n",
    "                                   dist.MultivariateStudentT(df=torch.cat((beta_prior_df, torch.tensor([1000], dtype=torch.float32))), \n",
    "                                                             loc=torch.cat((beta_prior_mean, \n",
    "                                                                            torch.tensor([0]*n_bins, dtype=torch.float32).expand((1, n_bins)))), \n",
    "                                                             scale_tril=torch.cat((beta_prior_L, \n",
    "                                                                                   torch.linalg.cholesky(torch.diag(1e-8*torch.ones(n_bins, dtype=torch.float32))).expand(1, n_bins, n_bins))))))\n",
    "        elif pdist==\"normal\":\n",
    "            beta_sel = pyro.sample(\"beta_sel\", dist.MixtureSameFamily(dist.Categorical(mix_probs),\n",
    "                                                                      dist.Normal(beta_prior_loc, beta_prior_scale).to_event(1)))\n",
    "        # apply transform to latent betas\n",
    "        if trans == \"abs\":\n",
    "            beta_trans = torch.cumsum(torch.abs(beta_sel), dim=-1)\n",
    "        elif trans==\"logabs\":\n",
    "            beta_trans = torch.cumsum(torch.log(torch.abs(beta_sel)+1), dim=-1)\n",
    "        elif trans==\"relu\":\n",
    "            beta_trans = torch.cumsum(relu(beta_sel), dim=-1)\n",
    "        elif trans==\"logrelu\":\n",
    "            beta_trans = torch.cumsum(torch.log(relu(beta_sel)+1), dim=-1)\n",
    "        \n",
    "    # calculate the multinomial coefficients for each gene and each mutation rate\n",
    "    mu_adj = mu_ref[...,None] * torch.cumsum(beta_prior_b, -1) * beta_trans[...,None,:]\n",
    "    mn_sfs = (beta_neut  - \n",
    "              beta_trans[...,None,:] -\n",
    "              mu_adj)\n",
    "    # convert to probabilities per-site and adjust for covariates\n",
    "    sfs = softmax(pad(mn_sfs[..., gene_ids, mu_vals, :] - torch.matmul(covariates, torch.cumsum(beta_cov, -1))))\n",
    "    \n",
    "    with pyro.plate(\"sites\", n_sites):\n",
    "        pyro.sample(\"obs\", dist.Categorical(sfs), obs=sample_sfs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
