import os
import sys
import glob
import numpy as np
import pandas as pd
import math
import sys
import random
import pickle
import csv

import dask.dataframe as dd
from dask.distributed import Client

import torch
import pyro
import pyro.distributions as dist
import pyro.distributions.constraints as constraints
from pyro.nn import PyroModule

from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader

sys.path.insert(0, '/home/djl34/lab_pd/kl/git/KL')
import raklette_updated
from run_raklette import run_raklette
from run_raklette import TSVDataset

##############################################################################################################

KL_data_dir = "/home/djl34/lab_pd/kl/data"
scratch_dir = "/n/scratch3/users/d/djl34"

base_set = ["A", "C", "T", "G"]
chrom_set = [str(x) for x in range(1, 23)]
# chrom_set = ["22"]

wildcard_constraints:
    chrom="\d+",
    epoch="\d+"

file_directory = "window_1k"

###################################################################################################################

file_names = glob.glob("/home/djl34/scratch/kl_input/window_1k/*_split_500*.tsv")

file_names = [x.split("/")[-1] for x in file_names]
file_names = [".".join(x.split(".")[:-1]) for x in file_names]

###################################################################################################################

rule all:
    input:
#         [os.path.join(KL_data_dir, "window_1_zscore/" + chrom +".tsv") for chrom in chrom_set],
#         [os.path.join(scratch_dir, "kl_input/" + file_directory + "/" + chrom +"_neutral.tsv") for chrom in chrom_set],
#         [os.path.join(scratch_dir, "kl_input/" + file_directory + "/" + chrom +"_neutral_summary.tsv") for chrom in chrom_set],
#         [os.path.join(scratch_dir, "kl_input/"+ file_directory + "/" + chrom +"_sorted.tsv") for chrom in chrom_set],
#         [os.path.join(scratch_dir, "kl_input/"+ file_directory + "/" + chrom +"_split_500_0.tsv") for chrom in chrom_set],
#         [os.path.join(scratch_dir, "kl_input/"+ file_directory + "/" + header + "_length.txt") for header in file_names],
#         [os.path.join(KL_data_dir, "raklette_output/gene_and_covariates/"+ file_directory + "/" + header +"_lr_0.01_gamma_0.1_epoch_3000.pkl") for header in file_names],
#         os.path.join(KL_data_dir, "raklette_output/gene_and_covariates/"+ file_directory + "/" + "summary_lr_0.01_gamma_0.1_epoch_3000.tsv"),
#         os.path.join(KL_data_dir, "raklette_output/gene_and_covariates/"+ file_directory + "/" + "summary_lr_0.01_gamma_0.1_epoch_3000_combined.tsv"),
        [os.path.join(KL_data_dir, "raklette_output/gene_and_covariates/"+ file_directory + "/" + header +"_lr_0.01_gamma_0.1_epoch_1000_nopriorfit.pkl") for header in file_names],


###################################### filter sites for KL analysis ######################################

# rule make_tsv_files:
#     input:
#         os.path.join(scratch_dir, "whole_genome/exon/{chrom}/_metadata")
#     output:
#         os.path.join(scratch_dir, "kl_input/" + file_directory + "/{chrom}.tsv")
#     run:
#         with Client() as client:

#             rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")

#             print(rate.columns)

#             include_columns = ["mu_index", "Freq_bin", "window_1k"]

#             rate = rate[include_columns]

#             rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
                        
rule get_window_zscore:
    input:
        os.path.join(KL_data_dir, "whole_genome/enhancer/{chrom}/_metadata"),
    output:
        os.path.join(KL_data_dir, "window_1_zscore/{chrom}.tsv")
    run:
        with Client() as client:

            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")

            include_columns = ["window_1k", "window_1k_zscore"]

            rate = rate[include_columns]
            
            rate = rate.drop_duplicates()  

            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)

rule sort:
    input:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "/{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "/{chrom}_sorted.tsv")
    run:
        with Client() as client:
            rate = dd.read_csv(input[0], sep = "\t")
            rate = rate.sort_values('window_1k') 
            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)

rule split_into_1k_genes:
    input:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "/{chrom}_sorted.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "/{chrom}_split_{maximum}_0.tsv")
    run:
        with Client() as client:
            
            f_in = open(input[0], "rt")
            in_reader = csv.reader(f_in, delimiter="\t")

            header = next(in_reader)
            header.insert(2, 'gene')
            
            counter = -1
            previous_window = -1
            i = 0
            
            f_out = open(output[0], "wt", newline="")
            out_writer = csv.writer(f_out, delimiter="\t", lineterminator="\n")
            out_writer.writerow(header)
            
            for row in in_reader:
                window = row[2]
                
                if previous_window != window:
                    counter += 1
                    previous_window = window
                    
                if counter == int(wildcards.maximum):
                    f_out.close()
                    i += 1
                    
                    new_filename = os.path.join(scratch_dir, "kl_input/" + file_directory + "/" + wildcards.chrom +"_split_500_" + str(i) + ".tsv")
                    print(new_filename)
                    
                    f_out = open(new_filename, "wt", newline="")
                    out_writer = csv.writer(f_out, delimiter="\t", lineterminator="\n")
                    out_writer.writerow(header)
                    
                    counter = 0    
                out_writer.writerow([row[0], row[1], counter, row[2]])
                
                
                
rule split_into_5k_genes:
    input:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "/{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "/{chrom}_split_5k_0.tsv")
    run:
        with Client() as client:

            rate = dd.read_csv(input[0], sep = "\t")

            maximum = rate["window_1k"].max().compute()

            max_iterator = int(maximum / split_by)

            print(str(maximum))

            for i in range(max_iterator + 1):
                rate_filtered = rate[(rate["window_1k"] < (i + 1) * split_by) & (rate["window_1k"] >= i * split_by)]

                rate_filtered = rate_filtered.compute()

                rate_filtered = rate_filtered.sort_values('window_1k')

                unique_list =list(rate_filtered["window_1k"].unique())

                unique_list.sort()

                rate_filtered["gene"] = [unique_list.index(x) for x in rate_filtered['window_1k']]

                rate_filtered[["mu_index", "Freq_bin", "gene", "window_1k"]].to_csv(os.path.join(scratch_dir, "kl_input/" + file_directory + "/" + wildcards.chrom +"_split_5k_" + str(i) + ".tsv"), sep = "\t", index = None)            
            
rule split_into_3k_genes:
    input:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "/{chrom}_split_5k_0.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "/{chrom}_split_2.5k_0.tsv")
    run:
        file_names = glob.glob(os.path.join(scratch_dir, "kl_input/" + file_directory + "/"+ wildcards.chrom + 
                                                "_split_5k_*.tsv"))
            
        iterator_list = [int(x.split(".")[0].split("_")[-1]) for x in file_names]

        maximum_value = max(iterator_list)

        output_iterator = 0

        for i in range(maximum_value + 1):
            df = pd.read_csv("_".join(file_names[0].split(".")[0].split("_")[:-1]) + "_" + str(i) + ".tsv", sep = "\t")

            df_0 = df[df["gene"] < 2500]
            df_1 = df[df["gene"] >= 2500]

            unique_list =list(df_1["window_1k"].unique())
            unique_list.sort()
            df_1["gene"] = [unique_list.index(x) for x in df_1['window_1k']]
            
            
            if len(df_0) > 0:
                df_0.to_csv(os.path.join(scratch_dir, "kl_input/" + file_directory + "/" + wildcards.chrom +"_split_2.5k_" + 
                                     str(output_iterator) +".tsv"), sep = "\t", index = None)
            
                output_iterator += 1
            
            if len(df_1) > 0:
                df_1.to_csv(os.path.join(scratch_dir, "kl_input/" + file_directory + "/"+ wildcards.chrom + "_split_2.5k_" + 
                                     str(output_iterator) +".tsv"), sep = "\t", index = None)
                output_iterator += 1

############################################# make site with no genes ##############################################
rule get_length:
    input:
        os.path.join(scratch_dir, "kl_input/{header}.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/{header}_length.txt")
    run:
        with Client() as client:
            rate = dd.read_csv(input[0], sep = "\t")
            
            length = len(rate)
            n_genes = len(rate["gene"].unique())
            
            f = open(output[0], "w")
            
            f.write(str(length) + "\n")
            f.write(str(n_genes) + "\n")
            
            f.close()
            
############################################# analysis for intergenic sites ##############################################
rule make_tsv_files_intergenic:
    input:
        os.path.join(KL_data_dir, "whole_genome/enhancer/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "/{chrom}_neutral.tsv")
    run:
        with Client() as client:

            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")

            print(rate.columns)

            include_columns = ["mu_index", "Freq_bin","window_1k", "Neutral"]

            rate = rate[include_columns]

            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
            
rule get_percentage_neutral:
    input:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "/{chrom}_neutral.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "/{chrom}_neutral_summary.tsv")
    run:
        with Client() as client:
            df = dd.read_csv(input[0], sep = "\t")
            df["Neutral"] = df["Neutral"].fillna(False).astype(int)
            df_window = df.groupby("window_1k")[["Neutral"]].sum().compute()
            df_window["size"] = df.groupby("window_1k").size().compute()
            df_window = df_window.reset_index()
            df_window.to_csv(output[0], sep = "\t", index = None)


    
                
########################################## For running KL analysis #####################################
        
rule run_KL_enhancer_module:
    input:
        variants = os.path.join(scratch_dir, "kl_input/{header}.tsv"),
        length_file = os.path.join(scratch_dir, "kl_input/{header}_length.txt"),
        neutral_sfs = KL_data_dir + "/whole_genome/neutral/5_bins/all.tsv"
    output:
        os.path.join(KL_data_dir, "raklette_output/gene_and_covariates/{header}_lr_{learning_rate}_gamma_{gamma}_epoch_{epoch}.pkl")
    run:
        n_covs = 0
        
        input_filename = input.variants
        output_filename = output[0]
        neutral_sfs_filename = input.neutral_sfs
        
        df = pd.read_csv(input.length_file, sep = "\t", header = None)
        nb_samples = df[0][0]
        n_genes = df[0][1]
        
        print("number of samples: " + str(nb_samples))
        print("number of genes: " + str(n_genes))
        
        if nb_samples == 0:
            f = open(output_filename, "w")
            f.write("no sample")
            f.close()
        else:        
            with open(input.variants) as f:
                first_line = f.readline()
            header = first_line.split("\t")
            
            chunksize = 1000000

            print("number of chunks " + str(nb_samples/chunksize))

            dataset = TSVDataset(input_filename, chunksize=chunksize, nb_samples = nb_samples, header_all = header, features = header)
            loader = DataLoader(dataset, batch_size=1, num_workers=1, shuffle=False)

            num_epochs = int(wildcards.epoch)

            #now with checkpoints
            run_raklette(loader, n_covs, n_genes, num_epochs, neutral_sfs_filename, output_filename, float(wildcards.learning_rate), float(wildcards.gamma))
        
rule post_analysis:
    input:
        [os.path.join(KL_data_dir, "raklette_output/gene_and_covariates/"+ file_directory + "/" + header +"_lr_0.01_gamma_0.1_epoch_3000.pkl") for header in file_names] 
    output:
        os.path.join(KL_data_dir, "raklette_output/gene_and_covariates/"+ file_directory + "/" + "summary_lr_0.01_gamma_0.1_epoch_3000.tsv")
    run:
        list_of_list = []
        
        print(len(input), flush=True)
        
        for i in range(len(input)):
            print(i, flush=True)
            filename = input[i]
            with open(filename, 'rb') as f:
                loaded_dict = pickle.load(f)
                
            filename = filename.split("/")[-1]
            
#             for j in range(len(loaded_dict["KL_fw_post"].mean(axis = 0))):
            kl_fw_list = ','.join(str(x) for x in loaded_dict["KL_fw_post"].mean(axis = 0))
            kl_rv_list = ','.join(str(x) for x in loaded_dict["KL_rv_post"].mean(axis = 0))

            list_of_list.append([filename, float(loaded_dict["beta_prior_df"]), kl_fw_list, kl_rv_list, loaded_dict["losses"][-1]])
            
        df = pd.DataFrame(list_of_list, columns = ["filename", "beta_prior_df", "KL_fw_post", "KL_rv_post", "loss"])
        df.to_csv(output[0], sep = "\t", index = None)
                          
rule combine_summary_data:
    input:
        os.path.join(KL_data_dir, "raklette_output/gene_and_covariates/"+ file_directory + "/" + "summary_lr_0.01_gamma_0.1_epoch_3000.tsv")
    output:
        os.path.join(KL_data_dir, "raklette_output/gene_and_covariates/"+ file_directory + "/" + "summary_lr_0.01_gamma_0.1_epoch_3000_combined.tsv")
    run:
        
        df = pd.read_csv(input[0], sep = "\t")
        df["KL_fw_post"] = df["KL_fw_post"].str.split(",")
        df["KL_rv_post"] = df["KL_rv_post"].str.split(",")
        
        first = True

        for index, row in df.iterrows():
            print(index, flush = True)
            df_row = pd.DataFrame(row).T

            filename = df_row["filename"].iloc[0]
            df_summary = pd.read_csv(scratch_dir + "/kl_input/window_1k/" +"_".join(filename.split("_")[0:4]) + ".tsv", sep = "\t")
            df_summary = df_summary[["gene", "window_1k"]].drop_duplicates()
            df_add = df_row.explode(['KL_fw_post', 'KL_rv_post'], ignore_index = True).reset_index(names = "gene")
            df_add = df_add.merge(df_summary, on = "gene", how = "left")
            
            if first:
                df_all = df_add
                first = False
            else:
                df_all = pd.concat([df_all, df_add])
                
        df_all["Chrom"] = df_all["filename"].str.split("_", expand = True)[0]
        
        first = True
        
        for chrom in chrom_set:
            df_all_chrom = df_all[df_all["Chrom"] == chrom]
            
            df_window_zscore = pd.read_csv(os.path.join(KL_data_dir, "window_1_zscore/" + chrom + ".tsv"), sep = "\t")
            df_all_chrom = df_all_chrom.merge(df_window_zscore, on = "window_1k", how = "left")
            
            if first:
                df_all_chrom.to_csv(output[0], sep = "\t", index = None)
                first = False
            else:
                df_all_chrom.to_csv(output[0], sep = "\t", index = None, mode='a', header=False)
                
            
########################################## For running KL analysis without fitting prior #####################################
        
rule run_KL_enhancer_module_noprior:
    input:
        variants = os.path.join(scratch_dir, "kl_input/{header}.tsv"),
        length_file = os.path.join(scratch_dir, "kl_input/{header}_length.txt"),
        neutral_sfs = KL_data_dir + "/whole_genome/neutral/5_bins/all.tsv"
    output:
        os.path.join(KL_data_dir, "raklette_output/gene_and_covariates/{header}_lr_{learning_rate}_gamma_{gamma}_epoch_{epoch}_nopriorfit.pkl")
    run:
        n_covs = 0
        
        input_filename = input.variants
        output_filename = output[0]
        neutral_sfs_filename = input.neutral_sfs
        
        df = pd.read_csv(input.length_file, sep = "\t", header = None)
        nb_samples = df[0][0]
        n_genes = df[0][1]
        
        print("number of samples: " + str(nb_samples))
        print("number of genes: " + str(n_genes))
        
        if nb_samples == 0:
            f = open(output_filename, "w")
            f.write("no sample")
            f.close()
        else:        
            with open(input.variants) as f:
                first_line = f.readline()
            header = first_line.split("\t")
            
            chunksize = 1000000

            print("number of chunks " + str(nb_samples/chunksize))

            dataset = TSVDataset(input_filename, chunksize=chunksize, nb_samples = nb_samples, header_all = header, features = header)
            loader = DataLoader(dataset, batch_size=1, num_workers=1, shuffle=False)

            num_epochs = int(wildcards.epoch)

            #now with checkpoints
            run_raklette(loader, n_covs, n_genes, num_epochs, neutral_sfs_filename, output_filename, float(wildcards.learning_rate), float(wildcards.gamma), fit_prior = False)
            