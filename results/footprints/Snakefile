## This is a snakemake to organize the dataframes for whole genomes
## I am making two different dataframes one will contain the features, the other will contain population sequencing data

import os
import sys
import glob
import numpy as np

import math
import sys
import random
import pickle

import pandas as pd
import dask
dask.config.set({'distributed.scheduler.allowed-failures': 0})
import dask.dataframe as dd
from dask.distributed import Client

sys.path.insert(0,'/home/djl34/lab_pd/bin')
import genomic

vep = "/home/djl34/bin/ensembl-vep/vep"

pd_data_dir = "/home/djl34/lab_pd/data"
KL_data_dir = "/home/djl34/lab_pd/kl/data"
scratch_dir = "/n/scratch/users/d/djl34/"

base_set = ["A", "C", "T", "G"]
all_chrom_set = [str(x) for x in range(1, 23)]
chrom_set = all_chrom_set
# chrom_set.remove("2")
# chrom_set.remove("3")
# chrom_set = ["19"]

print(chrom_set)

wildcard_constraints:
    chrom="\d+"
    
n = 0
    
def get_mem_mb(wildcards, attempt):
    return 10000 + (attempt + n) * 30000

def get_mem_mb_small(wildcards, attempt):
    return attempt * 10000

cwd = os.getcwd()
file_directory = "/".join(cwd.split("/")[-2:]) + "/"

# include: "/home/djl34/kl_git/preprocessing/whole_genome/Snakefile"

##############################################################################################################################   

mut_type_list = ["T_A", "T_C", "T_G", "C_G", "C_A", "C_T"]
    
## for epigenetic features
filename_list = [os.path.join(scratch_dir, "results/footprints/split/{chrom}.tsv")]

# filename_list = [os.path.join(scratch_dir, "results/method_validation/split/neutral/{chrom}.tsv")]

input_list = [filename.replace("{chrom}", str(chromosome)) for filename in filename_list for chromosome in chrom_set]

rule all:
    input:
        input_list,

################################################# creating data for results ##################################################
            
# create data for neutral sites
rule make_footprints:
    input:
        os.path.join(scratch_dir, "whole_genome/combined/footprints/{chrom}/_metadata")
    output:
        os.path.join(scratch_dir, "results/footprints/split/{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            filename = input[0]
            
            rate = dd.read_parquet("/".join(filename.split("/")[:-1]) + "/")
            
            rate["footprint_mean_signal"] = rate["footprint_mean_signal"].fillna(0)
            rate["DHS_mean_signal"] = rate["DHS_mean_signal"].fillna(0)
            
            # get footprints
            rate["footprints"] = 1
            rate["footprints"] = rate["footprints"].where((rate["footprint_mean_signal"] > 0), 0)
                        
            rate_footprints = rate[(rate["footprints"] == 1)]
            rate_footprints.to_csv(output[0], sep = "\t", single_file = True)
            
################################################# pivot to array ##################################################
rule pivot:
    input:
        [os.path.join(scratch_dir, "{header}/" + chrom +".tsv") for chrom in all_chrom_set],
    output:
        os.path.join(KL_data_dir, "{header}/pivot_bin_2.tsv"),
        os.path.join(KL_data_dir, "{header}/pivot_bin_9.tsv"),
        os.path.join(KL_data_dir, "{header}/pivot_bin_10.tsv"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_csv(input, sep = "\t")
            
            freq_breaks_2 = [-1, 1e-8, 0.5]
            freq_breaks_9 = [-1, 1e-8, 2e-06, 4.0e-06, 8.0e-06, 1.6e-05, 5e-05, 5e-04, 5e-03, 0.5] 
            freq_breaks_10 = [-1, 1e-8, 2e-06, 4.0e-06, 8.0e-06, 1.6e-05, 5e-05, 5e-04, 5e-03, 5e-02, 0.5] 

            rate["Freq_bin_2"] = rate['MAF'].map_partitions(pd.cut, freq_breaks_2, labels = False)
            rate["Freq_bin_9"] = rate['MAF'].map_partitions(pd.cut, freq_breaks_9, labels = False)
            rate["Freq_bin_10"] = rate['MAF'].map_partitions(pd.cut, freq_breaks_10, labels = False)
            
            
            columns_list = ['Freq_bin_2', 'Freq_bin_9', 'Freq_bin_10']
            for i in range(3):
                freq_bin = columns_list[i]
                
                rate_groupby = rate[["index", "mu_index", freq_bin]].groupby(["mu_index", freq_bin]).count().compute()
                rate_pivot = rate_groupby.reset_index().pivot(index='mu_index', columns=freq_bin, values='index')

                rate_pivot.to_csv(output[i], sep = "\t", index = None)   
                
rule divide_by_mut_type:
    input:
        [os.path.join(scratch_dir, "{header}/" + chrom +".tsv") for chrom in all_chrom_set],
    output:
        os.path.join(scratch_dir, "{header}/T_A.tsv"),
        os.path.join(scratch_dir, "{header}/T_G.tsv"),
        os.path.join(scratch_dir, "{header}/T_C.tsv"),
        os.path.join(scratch_dir, "{header}/C_G.tsv"),
        os.path.join(scratch_dir, "{header}/C_A.tsv"),
        os.path.join(scratch_dir, "{header}/C_T.tsv"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            def flip(base_pair):
                if base_pair == "T":
                    return "A"
                if base_pair == "A":
                    return "T"
                if base_pair == "G":
                    return "C"
                if base_pair == "C":
                    return "G"
                else:
                    return base_pair

            def flip_mutation_type(mut_type):
                if (mut_type[0] == "G") | (mut_type[0] == "A"):
                    output = ""
                    for i in mut_type:
                        output += flip(i)
                    return output
                else:
                    return mut_type
            
            rate = dd.read_csv(input, sep = "\t")
            
            rate["Allele_ref"] = rate["index"].str.split("_", n = 2, expand = True)[1]
            rate["Allele"] = rate["index"].str.split("_", n = 2, expand = True)[2]

            rate["mut_type"] = rate["Allele_ref"] + "_" + rate["Allele"]
            
            rate["mut_type"] = rate.apply(lambda row: flip_mutation_type(row["mut_type"]), axis=1, meta=pd.Series(dtype="string"))
            
            mut_type_list = ["T_A", "T_G", "T_C", "C_G", "C_A", "C_T"]
            
            columns_list = ['Freq_bin_2', 'Freq_bin_9', 'Freq_bin_10']
            for i in range(6):
                mut_type = mut_type_list[i]
                
                rate_mut_type = rate[rate["mut_type"] == mut_type]
                
                rate_mut_type.to_csv(output[i], single_file = True, sep = "\t", index = None) 

rule pivot_by_mut_type:
    input:
        os.path.join(scratch_dir, "{header}/{filename}.tsv"),
    output:
        os.path.join(KL_data_dir, "{header}/{filename}/pivot_bin_2.tsv"),
        os.path.join(KL_data_dir, "{header}/{filename}/pivot_bin_9.tsv"),
        os.path.join(KL_data_dir, "{header}/{filename}/pivot_bin_10.tsv"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_csv(input, sep = "\t")
            
            freq_breaks_2 = [-1, 1e-8, 0.5]
            freq_breaks_9 = [-1, 1e-8, 2e-06, 4.0e-06, 8.0e-06, 1.6e-05, 5e-05, 5e-04, 5e-03, 0.5] 
            freq_breaks_10 = [-1, 1e-8, 2e-06, 4.0e-06, 8.0e-06, 1.6e-05, 5e-05, 5e-04, 5e-03, 5e-02, 0.5] 

            rate["Freq_bin_2"] = rate['MAF'].map_partitions(pd.cut, freq_breaks_2, labels = False)
            rate["Freq_bin_9"] = rate['MAF'].map_partitions(pd.cut, freq_breaks_9, labels = False)
            rate["Freq_bin_10"] = rate['MAF'].map_partitions(pd.cut, freq_breaks_10, labels = False)
            
            
            columns_list = ['Freq_bin_2', 'Freq_bin_9', 'Freq_bin_10']
            for i in range(3):
                freq_bin = columns_list[i]
                
                rate_groupby = rate[["index", "mu_index", freq_bin]].groupby(["mu_index", freq_bin]).count().compute()
                rate_pivot = rate_groupby.reset_index().pivot(index='mu_index', columns=freq_bin, values='index')

                rate_pivot.to_csv(output[i], sep = "\t")  
                
rule ratio_by_mut_type:
    input:
        neutral = os.path.join(KL_data_dir, "results/method_validation/split/neutral/pivot_bin_10.tsv"),
        mut_types =[os.path.join(KL_data_dir, "{header}/"+ mut_type+ "/pivot_bin_10.tsv") for mut_type in mut_type_list]
    output:
        [os.path.join(KL_data_dir, "{header}/mut_type_ratios_" + str(i) +".tsv") for i in range(10)]
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=1,
        mem_mb=2000
    run:
        df_neutral= pd.read_csv(input.neutral, sep = "\t")
        df_neutral = df_neutral.div(df_neutral.sum(axis=1), axis=0)
        
        for i in range(10):

            df_output = df_neutral[[str(i)]].rename({str(i): "neutral_SFS"}, axis = 1)

            for mut_type in mut_type_list:
                filename = os.path.join(KL_data_dir, wildcards.header + "/"+ mut_type +"/pivot_bin_10.tsv")

                df= pd.read_csv(filename, sep = "\t")
                df = df.set_index("mu_index")

                df_output[mut_type + "_site_num"] = df.sum(axis = 1)

                df = df.fillna(0)

                df = df.div(df.sum(axis=1), axis=0)

                df_output[mut_type + "_ratio"] = df[str(i)] / df_neutral[str(i)]

            df_output.to_csv(output[i], sep = "\t")
